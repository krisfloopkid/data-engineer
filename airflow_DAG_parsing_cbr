"""
Парсит сайт CBR.ru
Складывает курс валют в Greenplum
Забирает из таблицы articles значение поля heading из строки с id, равным дню недели
Работает с понедельника по субботу
Даты работы дага: с 1 марта 2022 года по 14 марта 2022 года

"""

from airflow import DAG
from airflow.utils.dates import days_ago
import logging
from datetime import datetime
import csv
import xml.etree.ElementTree as ET
from airflow.operators.bash import BashOperator
from airflow.operators.python_operator import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook

DEFAULT_ARGS = {
    'owner': 'k-molodozhenova',
    'start_date': datetime(2022, 3, 2),
    'end_date': datetime(2022, 3, 15),
    'poke_interval': 60,
    'retries': 1
}

url = 'https://www.cbr.ru/scripts/XML_daily.asp?date_req={{ macros.ds_format(ds, "%Y-%m-%d", "%d/%m/%Y") }}'
table_gp = 'k_molodozhenova_cbr'
xml_path = '/tmp/k_molodozhenova_cbr.xml'
csv_path = '/tmp/k_molodozhenova_cbr.csv'


with DAG("k-molodozhenova_parsing_cbr",
          schedule_interval='0 0 * * MON-SAT',
          default_args=DEFAULT_ARGS,
          max_active_runs=1,
          tags=['km']
) as dag:

    extract_cbr_xml = BashOperator(
    task_id='extract_cbr_xml',
    bash_command=f'curl {url} | iconv -f Windows-1251 -t UTF-8 > {xml_path}'
    )
# =================================================================================
    def transform_xml_to_csv_func():
        parser = ET.XMLParser(encoding="UTF-8")
        tree = ET.parse(xml_path, parser=parser)
        root = tree.getroot()

        with open(csv_path, 'w') as csv_file:
            writer = csv.writer(csv_file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
            for Valute in root.findall('Valute'):
                NumCode = Valute.find('NumCode').text
                CharCode = Valute.find('CharCode').text
                Nominal = Valute.find('Nominal').text
                Name = Valute.find('Name').text
                Value = Valute.find('Value').text
                df = [root.attrib['Date']] + [Valute.attrib['ID']] + [NumCode] + \
                      [CharCode] + [Nominal] + [Name] + [Value.replace(',', '.')]
                writer.writerow(df)
                logging.info(df)

    transform_xml_to_csv = PythonOperator(
        task_id='transform_xml_to_csv',
        python_callable=transform_xml_to_csv_func
        )
#==================================================================================
    def load_csv_to_gp_func():
        pg_hook = PostgresHook('conn_greenplum')
        pg_hook.copy_expert(f"COPY {table_gp} FROM STDIN DELIMITER ','", csv_path)


    load_csv_to_gp = PythonOperator(
        task_id='load_csv_to_gp',
        python_callable=load_csv_to_gp_func
    )
# ==================================================================================

    remove_old_files = BashOperator(
        task_id='remove_old_files',
        bash_command=f'rm {csv_path} && rm {xml_path}'
    )

# ==================================================================================
    def select_heading_func(**kwargs):
        pg_hook = PostgresHook('conn_greenplum')
        week_day = datetime.strptime(kwargs['ds'], '%Y-%m-%d').weekday()+1
        logging.info(f'SELECT heading FROM articles WHERE id = {week_day}')
        conn = pg_hook.get_conn()
        cursor = conn.cursor("named_cursor_name")
        cursor.execute(f'SELECT heading FROM articles WHERE id = {week_day}')
        results = cursor.fetchall()
        return results

    select_heading = PythonOperator(
        task_id='select_heading',
        python_callable=select_heading_func
    )
# =================================================================================
    def delete_old_data_func(date_db):
        pg_hook = PostgresHook('conn_greenplum')
        conn = pg_hook.get_conn()
        cursor = conn.cursor()
        sql = f"""delete 
                from karpovcourses.public.{table_gp}
                where dt = '{date_db}';
                """
        cursor.execute(sql)
        conn.commit()
        conn.close()
        logging.info(f'Data from {date_db} successfully deleted')

    delete_old_data = PythonOperator(
        task_id='delete_old_data',
        python_callable=delete_old_data_func,
        op_args=[url]
    )
# ==================================================================================

extract_cbr_xml >> transform_xml_to_csv >> load_csv_to_gp >> remove_old_files
select_heading >> delete_old_data
